% !TEX TS-program = XeLaTeX
% !TEX encoding = UTF-8 Unicode

\chapter{分离效果的评价标准}

\label{chap03}

在使用独立成分分析算法产生了几组独立成分之后，需要将各组独立成分
与怀疑为影响因素的因子进行比较，考察它们之间的关系。可以使用的方法
有相关系数、互信息、灰色系统理论等。
 
\section{相关系数\ucite{PEARSON}}
在概率论和统计学中，\textbf{相关(Correlation)}，
显示两个随机变量之间线性关系的强度和方向。
在统计学中，相关的意义是用来衡量两个变量相对于其相互独立的距离。
在这个广义的定义下，有许多根据数据特点而定义的用来衡量数据相关的系数。

对于不同数据特点，可以使用不同的系数。
最常用的是\textbf{Pearson积差相关系数}。
其定义是两个变量协方差除以两个变量的标准差(方差的平方根)。
\begin{equation}
\rho_{X,Y} = {\mathrm{cov}(X,Y) \over \sigma_X \sigma_Y} 
           = {E((X-\mu_X)(Y-\mu_Y)) \over \sigma_X\sigma_Y}
\end{equation}
其中，$E$是\textbf{数学期望}，$cov$表示\textbf{协方差}。

因为$\mu_X = E(X)</math>, <math>\sigma_X^2 = E(X^2) - E^2(X)$，
同样地，对于$Y$，可以写成
\begin{equation}
\rho_{X,Y} = 
       \frac{E(XY)-E(X)E(Y)}
            {\sqrt{E(X^2)-E^2(X)}~\sqrt{E(Y^2)-E^2(Y)}}
\end{equation}
当两个变量的标准差都不为零时，相关系数才有定义。
从柯西-施瓦茨不等式可知，相关系数的绝对值不超过1。
当两个变量的线性关系增强时，相关系数趋于1或-1。
当一个变量增加而另一变量也增加时，相关系数大于0。
当一个变量的增加而另一变量减少时，相关系数小于0。

当两个变量独立时，相关系数为0.但反之并不成立。 
这是因为相关系数仅仅反映了两个变量之间是否线性相关。
比如说，$X$是区间$[-1, 1]$上的一个均匀分布的随机变量。
$Y$ = $X^2$。 那么$Y$是完全由$X$确定。因此$Y$ 和$X$是不独立的。
但是相关系数为0。或者说他们是不相关的。
当$Y$和$X$服从联合正态分布时，其相互独立和不相关是等价的。

当一个或两个变量带有测量误差时，他们的相关性就受到削弱，
这时，\textbf{反衰减性(disattenuation)}是一个更准确的系数。


\section{互信息\ucite{AAPO01}}
互信息是一组随机变量的成员具有的该集合中其他随机变量的信息的量度。(***)

\subsection{使用熵的定义(***)}
可以利用熵，
将$n$个(标量)随机变量$x_i, i=1,\cdots,n$之间的互信息$I$定义为：
\begin{equation} \label{mutual_info_defination1}
I(x_1, x_2,\cdots ,x_n)=\sum^n_{i=1} H(x_i)-H(\bm{x})
\end{equation}
式中，$\bm{x}$是包含所有$x_i$的那个变量。

互信息可以像熵一样解释成代码长度。
$H(x_i)$这些项给出了当$x_i$这些变量各自单独编码时的码长，
而$H(\bm{x})$给出了当$x$作为一个随机向量即
所有分量都编码在同一个代码中时的码长。
这样，互信息表明了，通过对整个变量进行编码，
相对于对单个分量分别编码所获得的码长的减少量。
一般而言，对整个向量编码会得到更好的代码。
然而，如果$x_i$之间相互独立，它们都不给出对方的信息，
这时我们只需对各个变量分别单独编码，而不增加码长。

\subsection{使用Kullback-Leibler散度的定义(***)}
换另一种观点。利用所谓的Kullback-Leibler散度，
可以把互信息解释成一个距离。
两个$n$维概率密度函数$p^1$和$p^2$之间的Kullback-Leibler距离定义为：
\begin{equation}
\delta(p^1, p^2) = \int p^1(\xi) log \frac{p^1(\xi)}{p^2(\xi)}\,d\xi
\end{equation}
Kullback-Leibler距离可以看成两个概率密度之间的一种距离，
因为它总是非负的。它等于零，当且仅当这两个密度相等。
这是负对数的(严格)凸性和应用经典的Jensen不等式的一个直接推论。
Jensen不等式(***)断言，对于严格凸函数$f$和任一随机变量$y$，
我们有
\begin{equation}
E\{f(y)\} \ge f(E\{y\})
\end{equation}
取$f(y)=-log(y)$，并假设$y=p^2(x)/p^1(x)$，
其中$x$具有由$p^1$给出的分布。这时我们有：
\begin{multline}
\delta(p^1, p^2) = E\{-log \frac{p^2(x)}{p^1(x)} \} 
	= \int p^1(\xi)\{-log \frac{p^2(x)}{p^1(x)} \} 
    \ge f(E\{y\}) \\ 
    = -log \int p^1(\xi)\{ \frac{p^2(x)}{p^1(x)}\}\,d\xi
    = -log \int p^2(\xi)\,d\xi = 0
\end{multline}
进而，在Jensen不等式中等号成立当且仅当$y$是常数。
在这种情况下，它是常数，当且仅当这两个密度相等，
因而，我们证明了前面宣称的Kullback-Leibler散度的性质。

然而，Kullback-Leibler散度并不是一个真正的距离量度，因为它不是对称的。

为了在这里应用Kullback-Leibler散度，
我们将从考虑各随机变量$x_i$之间相互独立的情形开始，
按照独立性的定义，它们的联合概率密度可以分解。
于是，可以用实际密度$p^1=p_x(\zeta)$和分解的密度
$p^2=p_1(\zeta_1)p_2(\zeta_2)\cdots p_n(\zeta_n)$
之间的Kullback-Leibler散度作为$x_i$之间独立性的度量，
其中，$p_i(.)$是$x_i$的边缘密度。事实上，通过简单的代数运算，
可以说明，这个量就等于我们在式(\ref{mutual_info_defination1})中用熵定义的互信息。

Kullback-Leibler散度的定义互信息蕴含了下面的重要性质：
\textbf{互信息是非负的，而它等于零当且仅当各变量相互独立}。
这是Kullback-Leibler散度性质的直接推论。
 

\section{灰色系统理论\ucite{DEFORM03}}
\subsection{概述}
灰色系统理论是由我国原华中理工大学郑聚龙教授在20世纪80年代提出的，
它是用来解决信息不完备系统的数学方法，
它把控制论的观点和方法延伸到复杂的大系统中，
将自动控制与运筹学的数学方法相结合，用独树一帜的方法和手段，
研究了广泛存在于客观世界中具有灰色性的问题。
在短短的时间里灰色系统理论有了飞速的发展，
它的应用已渗透到自然科学和社会经济的许多领域，
显示出这门学科的强大生命力，具有广阔的发展前景。

系统分析的经典方法是将系统的行为看做是随机变化的过程，
用概率统计的方法，从大量数据中寻找统计规律，
这对于统计数据量较大情况下得处理较为有效，
但对于数据量少得贫信息系统的分析则较为棘手。

灰色系统理论研究的是贫信息建模，
它提供了贫信息情况下解决系统问题的新途径。
它把一切随机过程看做是在一定范围内变化的、与时间有关的灰色过程，
对灰色量不是寻找统计规律的角度。通过大样本进行研究，
而是用数据生成的方法。
将杂乱无章的原始数据整理成规律性较强的生成数列后再做研究。
灰色理论认为系统的行为现象尽管是朦胧的，数据是杂乱无章的，
但它毕竟是有序的，有整体功能的，在杂乱无章的数据后面，
必须潜藏着某种规律，灰数的生成，
是从杂乱无章的原始数据中去开拓、发现、寻找这种内在的规律。

\subsection{灰色系统理论的基本概念}
\paragraph*{1. 基本概念}
\subparagraph*{灰色系统}
信息不完全的系统称为灰色系统。
信息不完全一般指：系统因素不完全明确；因素关系不完全清楚；
系统结构不完全知道；系统的作用原理不完全明了。
\subparagraph*{灰数、灰元、灰关系}
灰数、灰元、灰关系是指灰色现象的特征，是灰色系统的标志。
灰数是指信息不完全的数，即只知大概范围而不知其确切值的数，
灰数是一个数集，记为$\otimes$；灰元是指信息不完全的元素；
灰关系是指信息不完全的关系。
\subparagraph*{灰数的白化值}
所谓灰数的白化值是指，令$a$为区间，$a_i$为$a$中的数，
若$\otimes$在$a$中取值，则称$a$为$\otimes$的一个可能的白化值。
\subparagraph*{数据生成}
将原始数据列$x$中得数据$x(k), x=\{x{k}|k=1,2,3,\cdots,n\}$，
按某种要求作数据处理称为数据生成。如建模生成与关联生成。
\paragraph*{2. 累加生成与累减生成}
累加生成与累减生成是灰色系统理论与方法中占据特殊地位的两种数据生成方法，
常用于建模，亦称建模生成。

累加生成(Accumulated Generation Operation，AGO)，
即对原始数据列中各个时刻的数据依次累加，从而形成新的序列。

设原始数列为
\begin{equation}
x^{(0)}=\{x^{(0)}(k)|k=1,2,\cdots,n\}
\end{equation}
对$x^{(0)}$作一次累加生成($1-AGO$)
\begin{equation}
x^{(1)}=\sum^k_{i=1}x^{(0)}(i)
\end{equation}
即得到一次累加生成序列
\begin{equation}
x^{(1)}=\{x^{(1)}|k=1,2,\cdots,n\}
\end{equation}
若对$x^{(0)}$作$m$次累加生成(记作$m-AGO$)，则有
\begin{equation}
x^{(m)}(k)=\sum^k_{i=1} x^{(m-1)}(i)
\end{equation}

累减生成(Inverse Accmulated Generating Operation,IAGO)是AGO得逆运算，即对生成序列的前后两数据进行差值运算。
\begin{gather}
x^{(m-1)}(k)=x^{(m)}(k)-x^{(m)}(k-1) \notag \\
\cdots\cdots \\
x^{(0)}(k)=x^{(1)}(k)-x^{(1)}(k-1)  \notag
\end{gather}
m-AGO和m-IAGO的关系是：
\begin{equation}
x^{(0)} \quad \xleftrightarrow[m-IAGO]{m-AGO} \quad x^{(m)}
\end{equation}

\subsection{灰色关联分析}
由灰色系统理论提出的灰关联度分析方法，
是基于行为因子序列的微观或宏观几何接近，以分析和确定因子间
的影响程度或因子对主行为的贡献测度而进行的一种分析方法
。灰联度是指事物之间的不确定性关联，
或系统因子与主行为因子之间不确定性关联。
它根据因素之间发展态势的相似或相异程度来衡量因素间的关联程度。
由于关联度分析是按发展趋势作分析，因而对样本量的大小没有太高的要求，
分析时也不需要典型的跟不规律，而且分析的结果一般与定性分析相吻合，
具有广泛的实用价值。

\subsubsection*{1. 构造灰关联因子集}
对抽象系统进行关联分析时，首先要确定表征系统特征的数据列。
表征系统有直接法和间接法两种。
直接法指对直接能得到反映系统行为特征的序列，可直接进行灰关联分析；
间接法指对不能直接找到表征系统的行为特征数列。
这就需要寻找表征系统行为特征的间接量，称为映射量，然后用此映射量进行分析。

在灰色系统理论中，确定表征系统特征的序列，并对数据进行处理，
称为构造灰关联因子集。灰关联因子集是灰关联分析的重要概念，
一般来说，进行灰关联分析时，都要把原因因子转换为灰关联因子集。

设时间序列(原始序列)
\begin{equation}
x=\{x(k)|k=1,2, \cdots ,n\}
\end{equation}
常用的转换方法有以下6种： \\
1) 初值化
\begin{equation}
x'(k)=\frac{x(k)}{x(1)}, \quad k=1,2, \cdots, n
\end{equation}
2) 平均值化
\begin{equation}
x'(k)=\frac{x(k)}{\frac{1}{n} \sum_{k=1}^n x(k)}, \quad k=1,2, \cdots, n
\end{equation}
3) 最大值化
\begin{equation}
x'(k)=\frac{x(k)}{\underset{k}{\max} x(k)}, \quad k=1,2, \cdots, n
\end{equation}
4) 最小值化
\begin{equation}
x'(k)=\frac{x(k)}{\underset{k}{\min} x(k)}, \quad k=1,2, \cdots, n
\end{equation}
5) 区间值化 \\
考虑 $x_i=\{x_i(k)|k=1,2, \cdots ,n\}, i=1,2, \cdots ,n$ \\
令 $\max\max X=\underset{i}{\max} \underset{k}{\max} x_i(k)$，\\
则
\begin{equation}
x_i'(k)=\frac{x_i(k)-\min\min X}{\max\max X - \min\min X}
\end{equation}
6) 正因子化 \\
令 $X_{\min}=\underset{k}{\min}x(k)$
\begin{equation}
x'(k)=x(k)+2|X_{\min}|, \quad (k=1,2, \cdots, n)
\end{equation}

\subsubsection*{2. 灰关联度的计算公式} 
设$x_0=\{x_0(k)|k=1,2, \cdots ,n\}$为参考序列；
$x_i=\{x_i(k)|k=1,2, \cdots ,n\}$为比较序列。则有如下定义：

$x_i(k)$与$x_0(k)$的关联系数为：
\begin{equation}
\xi_i(k)=\frac{\underset{i}{\min}\underset{k}{\min}|x_0(k)-x_i(k)|
				+\rho\underset{i}{\max}\underset{k}{\max}|x_0(k)-x_i(k)|}
			  {|x_0(k)-x_i(k)|
				+\rho\underset{i}{\max}\underset{k}{\max}|x_0(k)-x_i(k)|}
\end{equation}
式中，$\rho$为分辨系数，$\rho$越小分辨率越大，
一般$\rho$的取值区间为$[0,1]$，通常取$\rho=0.5$。

于是，可求出$x_i(k)$和$x_0(k)$的关联系数
\begin{equation}
\xi_i=\{\xi_i(k)|k=1,2,\cdots,n\}
\end{equation}
则灰关联度定义为：
\begin{equation}
\gamma_i=\gamma(x_0,x_i)=\frac{1}{n}\sum_{k=1}^{n}\xi_i(k)
\end{equation}
灰关联度具有如下特性： \\
1) 规范性
\begin{gather}
0 < \gamma(x_0,x_i) \le 1 \notag \\
\gamma(x_0,x_i) = 1 \Leftrightarrow x_0=x_i \\
\gamma(x_0,x_i) = 0 \Leftrightarrow x_i,x_0 \in \varnothing \notag
\end{gather}
2) 偶对称性
\begin{equation}
\gamma(x,y)=\gamma(y,x), x,y \in x
\end{equation}
3) 整体性 \\ 
若$x_i(i=1,2,\cdots,m) m \ge 3$ ，则一般地有
\begin{equation}
\gamma(x_i,x_j) \neq \gamma(x_j, y_i),\quad i\neq j,\quad i,j=1,2,\cdots,n
\end{equation}
4) 接近性 \\
$\Delta_i(k)=|x_0(k)-x_i(k)|$越小，则$\gamma(x_0,x_i)$越大，
即$x_0$与$x_i$越接近。
\\

从上述灰关联度的性质(3)可以看出，
灰关联度一般不满足对称性，
于是便有了如下满足对称性的灰关联度计算公式：\\
①改进关联度法
\begin{equation}
r_{ij}=\frac{1}{2(n-1)} \left[
\frac{x_i(i)\wedge x_j(1)}{x_i(1) \vee x_j(i)}+
\frac{x_i(n)\wedge x_j(n)}{x_i(n) \vee x_j(n)}+
2\sum_{k=2}^{n-1}\frac{x_i(k)\wedge x_j(k)}{x_i(k)\vee x_j(k)}
\right]
\end{equation}
②相对变率关联度法
\begin{equation}
r_{ij}=\frac{1}{n-1}\sum_{k=1}^{n-1} \frac{1}{1+|
\frac{\Delta x_j(k)}{x_j(k)}-\frac{\Delta x_i(k)}{x_i(k)}|}
\end{equation}
式中，$\Delta x_j(k)=x_j(k+1)-x_j(k), \Delta x_i(k)=x_i(k+1)-x_i(k)$ \\
③斜率关联度法
\begin{equation}
r_{ij}=\frac{1}{n-1}\sum_{k=1}^{n-1} \frac{1}{1+|
\frac{\Delta x_j(k)}{\sigma_{x_j}}-\frac{\Delta x_i(k)}{\sigma_{x_i}}|}
\end{equation}
式中，
\begin{gather}
\sigma_{x_j}=\sqrt{\frac{1}{n-1}\sum_{k=1}^{n}(x_j(k)-\bar{x}_j)^2}
\qquad ; \qquad \bar{x}_j=\frac{1}{n} \sum_{k=1}^{n} x_j(k)  \notag \\
\sigma_{x_i}=\sqrt{\frac{1}{n-1}\sum_{k=1}^{n}(x_i(k)-\bar{x}_i)^2}
\qquad ; \qquad \bar{x}_i=\frac{1}{n} \sum_{k=1}^{n} x_i(k)  \notag 
\end{gather}
   
\subsubsection*{3. 关联序}
设参考序列$x_0$与比较序列$x_i (i=1,2,\cdots,m)$，
其关联度分别为$\gamma_i(i=1,2,\cdots,m)$ ，
按关联度大小排序即为关联序。

在灰关联分析中，关联序的大小体现了比较因子对参考因子的影响及作用的大小，
其意义高于关联度本身的大小。

需要指出的是，在关联度的分析中，数列的处理方法不同，
关联度的大小会发生变化，但关联序一般是不会发生变化的。
也就是说，关联度的大小只是因子之间相互影响、相互作用的外在表现，
而关联序才是其实质。

