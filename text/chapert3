三 独立分量分析

3.1 盲源分离
    3.1.1 盲源分离问题的提出
    让我们来考虑这样的一般情况：有这么一组信号，是由几个物理
对象或物理源发出的，物理源可以是发出电信号的不同脑区，可以是
同一房间讲话的人，也可能是发射无线电波的移动电话。进一步假设
存在多个传感器或接收机，而这些传感器安置在不同的位置，从而，
每个传感器可以分别以略为不同的权重记录各物理源信号的某种混合。
    为使问题的陈述更为简单，我们假定有三个信号源，同时有三个
观测信号。把观测信号记为 x1(t)，x2(t)和x3(t)，它们是所记录的
信号在t时间点处的幅值；原始信号记为s1(t)，s2(t)和s3(t)。
这样xi(t)是si(t)的加权和，而加权系数依赖于源和传感器之间的距离：
  xxxxxxxxxxxxxx
  xxxxxxxxxxxxxxxxxxx
  xxxxxxxxxxxxx
式中，aij是常值系数，表示混合的权重。aij是未知的，因为我们不可能
了解物理混合系统的全部特性(这通常是极其困难的)，所以我们无法知道
aij的值。源信号si也同样是未知的，而这正是要解决的问题：因为我们
不能对它们进行直接记录。
    我们想要做的就是利用x1(t)，x2(t)和x3(t)这些混合量找出原始信号
这就是盲源分离(BSS)问题。盲意味着我们对原始信号所知甚少。
    不妨假设混合系数aij具有足够的差异，使得他们构成的矩阵可逆。
因此构成的矩阵可逆。因此存在一个以元素wij为系数的矩阵w，使得我们
可以用它分离出源信号si：
    s1(t)=w11x1(t)+w12x2(t)+w13x3(t)
	s2(t)=w21x1(t)+w22x2(t)+w23x3(t)
	s3(t)=w31x1(t)+w32x2(t)+w33x3(t)
	如果我们已经知道公式(****)中的那些系数aij，将它们形成的矩阵求逆
即可得到矩阵W。
    
    3.1.2 基于独立性的源分离
    现在的问题是：如何估计公式(*****)中的系数wij？我们希望获得一种
普适性的方法，使它能适合许多不同的场合，并给最开始提出的问题————
即为多元数据寻找一个好的表示法，提供一种答案。但是我们只能使用非常
一般的统计性质，因为x1,x2,x3是我们的全部观测。我们还希望找到一个
矩阵W，使得这个好的表示法可以用源信号s1,s2和s3给出。
    仅仅通过考虑信号的统计独立性，就可以找到上述问题的一个令人惊奇
简单求解方式。事实上如果信号是非高斯的，那么只需确定系数wij，使得
信号：
    y1(t)=w11x1(t)+w12x2(t)+w13x3(t)
	y2(t)=xxxxxxxxxxxxxxxxxxxxxxxxxxx
	xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
是统计独立的即可。如果信号y1，y2和y3是统计独立的，那么它们
就等同于原始信号s1,s2和s3(他们可能是某种标量常数乘积的关系
也就是说一个信号可能是另一个信号乘以一个标量的比例常数，
但这并不重要)。
    事实上，仅仅利用统计独立性的信息，我们就可以估计出图(****)
中信号所对应的系数矩阵W，从而得到如图(*****)所示的源信号
(这些信号是用后文提到的FastICA算法估算出来的)。可以看到，
从一个貌似噪声的数据集中，利用一个只用到统计独立性信息的算法
就能将源信号估计出来。而估计得到的信号确实等于我们用于产生
图(****)中混合信号的源信号(源信号没有给出，不过它们确实与算法
找出的信号在本质上是等价的)。而在源分离的问题中，原始信号就是
数据集的“独立成分”。
    

3.2 独立成分分析
    3.2.1 定义
	前面我们已经看到，盲源分离问题可以归结为寻找一个线性表示，使得
该表示对应的成分统计独立。在实际情形下，我们一般不可能找到一个其成
真正独立的表示，但是至少能够找到一个其成分尽可能独立的表示。
    这使我们能够对ICA进行这样简单的定义(后面我们将从另外的角度给出
更加详细的定义)：
	给定随机变量的一组观测(x1(t)，x2(t)，...,xn(t))，其中t是时间或者
样本标号，假设它们由独立成分线性混合：
    xxxxxxxxxxxxxx
	xxxxxxxxxxxxx
	xxxxxxxxx
式中，A是某个未知矩阵。我们仅能观察到xi(t)的情况下，独立成分分析就要
同时估计出矩阵A和si(t)。注意此处我们还鉴定独立成分si(t)的数目与观测
变量的数目相同；其实这只是一个简化假设，而不是必须的。我们可以将ICA
定义为另外一种形式：寻找一个类似与公式(***)中矩阵W确定的线性变换，
使得随机变量yi(i=1...n)尽可能独立。这种表述和前一种表述并没有很大差异，
因为如果矩阵W能估计出，对其求逆就能得到矩阵A。
    可以表明，该问题是适定的，也就是说公式(***)中的模型可估，而且仅当
各成分si是非高斯性的，这是一个具体要求。关于对于为什么要求非高斯性的
讨论，可以参考(*****)。
    关于ICA更加严格的定义，请参考(****)和(****)
    3.2.2 如何寻找独立成分
	在除独立性外没有其他任何假设的情况下，仍能从线性混合中估计出独立
成分，这个结论可能让人觉得非常吃惊。那么下面我们试图简要地回答上述情况
为什么是可能的，以及如何实现两个基本疑问。
    *仅仅不相关是不够的*：首先需要注意的是，独立性是比不相关性强得多的性质。
对于盲源分离问题，我们实际上可以找到信号的许多不同的不相关表示法，
但这些表示未必独立，也未必能将源信号分离出来。不相关性就其本身而言
是不足以分离这些成分的。这也是主成分分析或因子分析不能分离信号的
原因：它们给出的成分除了不相关外就没有更多信息了。
    事实上，利用我们熟知的去相关方法，可以将独立成分的任何线性混合变换
成不相关成分，其中的混合变换是正交的(参考****)。这样，ICA的要点就是估计
去相关之后留下的未知正交矩阵。这是经典方法所不能估计的，因为它们和去相关
方法一样，是基于协方差信息的。
    
	*非线性去相关是基本ICA方法*：表达独立性如何强于不相关行的一种说法是，
独立性本事就蕴含了*非线性不相关性*：若s1和s2独立，那么任何非线性变化g(s1)
和h(s2)都是不相关的(在它们之间协方差为零的意义下)。与此形成鲜明对比的是，
对于两个仅仅不相关的随机变量，这样两个非线性变换一般不再具有零协方差。
这样我们可以通过一种更强形式的去相关运算来实现ICA：即，寻找一个表示，使得
yi即使通过非线性变化仍然不相关。这给出了估计矩阵W的一个简单原理：
    ICA估计原理1：非线性去相关。寻找矩阵W，是的对任何i！=j，成分yi和yj
	不相关，而且变换之后的成分g(yi)和h(yj)也不相关，其中，g和h是某些
	适当的非线性函数。

	这是估计ICA的一个有效的方法：如果非线性函数选得适当，此方法的确能找到
独立成分。虽然这个原理非常直观，但她却遗留了一个重要的问题：非线性函数
g和h如何选择？该问题的答案可以从估计理论和信息论中找到。估计理论提供了可以
估计任何一个统计模型的最为经典的方法：极大似然估计(参见***)。信息论可以给出
独立性的一些准确度量，如互信息(参见***)。利用其中任何一种理论，我们都能
确定出满意的非线性函数g和h。
    
    *独立成分是极大非高斯性成分*：ICA估计另一个非常直观和重要的原则是
极大非高斯性(参加***)。其思路是，根据中心极限定义，非高斯性随机变量之和
比原变量更加接近高斯变量。因此，如果我们取观测混合变量的一个线性组合
y=sigma-i(bi*xi)(因为混合模型是线性的，因此该线性组合同时也是独立成分
的一种线性组合)，如果它等于独立成分之一，那么它的非高斯性达到极大。
这是因为，如果它确实是两个或更多成分的混合，按中心极限定义，该混合将
更接近高斯分布。
    这样，相关的愿意可以表述为：

	*ICA估计原理2*：极大高斯性。在y的方差为常数的约束下，球现行组合
	y=sigma-i(bi*xi)非高斯性的局部极大值。每个局部极大值给出一个独立成分。
    
	为了在实际应用中度量非高斯性，我们可以使用一些变量，比方说*峭度*。
峭度是一个高阶累积量，它是方差的某种推广(利用高阶多项式)。累积量具有一些
有趣的代数和统计性质，这也是它们在ICA理论中起着重要作用的原因。
    有意思的一点是，极大非高斯性原理表明了ICA和独立发展技术，称为*投影寻踪*
的这项技术之间的紧密关系。在投影寻踪方法里，我们实际上也是寻找具有极大
非高斯性的线性组合，并用于可视化或其它目的。这样独立成分可以解释成投影寻
寻踪的方向。
    当ICA用于提取特征时，极大非高斯性原理也表明了它与在特征提取的神经科学
理论中使用过的*稀疏编码*之间的紧密关系(参看***)。稀疏编码的思路是将数据用
成分表示，使得只有很少数量的成分是同时激活的。在某些情形下，这等价于寻找
极大非高斯性成分。
    ICA与投影寻中和稀疏编码之间的这些联系，都和一个更为深入的结果有关，
该结果表明，ICA给出了一个*尽可能结构化*的线性表示。此论断可以用信息论概念
给出其严格的意义(参见第10章)，并且相关结果还表明，独立成分在许多方面比
原始随机变量更容易处理。特别地，独立成分比原始变量更加容易编码(压缩)。
    
    *ICA估计所需信息比协方差更多*：还有很多其它方法也可以估计ICA模型。
这些方法的共同点是，它们考虑了没有包含在协方差矩阵中的某些统计量。
(协方差矩阵包含的是所有xi对之间的协方差)。
    利用协方差矩阵，我们可以在通常线性意义下取出各成分间的相关性，
但仅此而已。故所有ICA方法都用到了某种形式的*高阶统计量*，高阶特别地
意味着这些信息并未包含在协方差矩阵中。到此为止我们已经遇到过了两类
高阶信息：非线性相关性和峭度。也可以使用其它很多类型的高阶统计量。
    
    *数值方法是重要的*：除了估计原理外，我们还必须找到一个具体算法，
以实现所需的计算。由于估计原理使用的是非二次的函数，所需要的计算通常
不能用简单的线性代数来表达，因此算法方面的要求可能是很高的。这样，
数值算法就成为ICA估计方法一个不可缺少的组成部分。
    数值方法通常是基于某种目标函数的优化。基本的优化方法是梯度法，
而特别有意思的是一个称为FastICA的不动点算法，它似乎是特别为ICA问题
量身定制的，可以充分地挖掘问题的特殊结构。我们可以采用两种方法任意
一个找到用峭度绝对值度量的非高斯性极大解。
    
    3.2.3 ICA的约束
	为了确保上面给出的基本ICA模型能被估计，我们必须作出一定的假设和约束。
	1).独立成分被假定是统计独立的
	该假设是ICA能够成立的前提，但令人惊讶的是，为保证模型能够被估计，仅仅
这个约束已经基本足够了，这就是ICA能在许多不同领域的应用中，成为一个强有力
的方法的原因。
    如果从基本概念上理解，我们说，随机变量y1,...,yn是独立的，是指在i!=j时，
有关yi的取值情况对于yj如何取值没有提供任何信息。从技术角度，独立可以通过
概率密度来定义。定义p(y1,y2,...,yn)为yi的联合概率密度函数，pi(yi)为yi的
边缘概率密度函数(即只考虑yi本身的概率密度函数)。那么我们说yi是独立的，
当且仅当联合概率密度函数可以因式分解为下面的形式：
      xxxxxxxxxxxxxxxxxxx
	2) 独立成分必须具有非高斯的分布
	我们通常直观地说高斯分布“太简单”了。高斯分布所有高阶累积量都为零，
但是这样的高阶信息对于估计ICA模型来说却是必须的。因此，如果观测变量具有
高斯分布，那么ICA在本质上是不可能实现的。值得注意的是，在基本模型中，我们
没有假定已知那些独立成分的的非高斯分布的样子，否则问题就会相当简单。
    3) 为简单起见，我们假定未知的混合矩阵是方阵
	换句话说，就是独立成分的个数与观测到的混合量的个数是相同的。对于进一步的
讨论来说，这个假设在某些情况下是不严谨的。但这里做该假设可以大大简化估计
过程，另外得到矩阵A的估计后，可以计算它的逆(用B表示)，并简单地通过下式
得到其独立成分：
      s=Bx
这里同时也假定了混合矩阵是可逆的。如果不是i这样，就说明存在可以忽略的冗余
混合量，这样矩阵就不会是方阵，也就说说混合量的个数与独立成分的个数是不相等的。
    在上面三个假设(或至少第1和第2两个假设)的前提下，ICA模型是可辨识的，
意指混合矩阵与那些独立成分可以被估计至存在一些平凡不确定性(trivial indetterminacies)
的程度。对于ICA模型的可辨识性的证明，请参考(***)。同时(****)中也为可辨识性
给出了一种不甚严格但可以帮助理解的构造性证明。
      
    3.2.4 ICA中的含混因素
	根据以上所定义的ICA模型，很容易发现存在下面一些必然的含混因素或者说，不确定性。
	1) 无法确定独立成分的方差(能量)
	原因是，s和A都是未知的，对于某一个源的任意标量乘积都能通过对A矩阵对应的列ai
除以对应的标量值ai而抵消：
      xxxxxxxxxxxxxx
作为推论，我们也可以用同样的方式对独立成分的幅值进行修正。因为上述分类都是随机变量，
进行修正最自然的方式就是假定它们都具有单位方差：E{s_i^2}=1。然而在ICA求解方法，
可以通过调整矩阵A来实现该归一化约束。不过即使这样，仍然遗留了符号的不确定性问题：
我们可以通过对某一独立成分乘以-1而不影响模型。不过幸运的是这种汗混因素在绝大多数
的应用中是无关紧要的。
    2) 无法确定独立成分的次序
	这仍然是s和A均为未知造成的，我们可以任意交换求和式中各项的次序，也可以
把任意一个独立成分作为第一个。从形式上来说，就是可以用一个置换矩阵机器逆代入模型，
得到x=AP^(-1)Ps。Ps矩阵中的元素萦绕是原来的独立变量sj，但是次序发生了变化。
而矩阵AP^(-1)则是另一个新的需要通过ICA算法求解的混合矩阵。
 
    3.2.5 中心化
	不失一般性，我们可以假定所有的混合变量与独立成分具有零均值。这样可以相当程度地
简化理论和算法，并作为一个默认的假设。
    实际情况中，往往不满足零均值假设，我们可以进行一些预处理使其满足该要求，这可以
通过对可观测变量进行中心化(centering)实现。就是说在实施ICA之前，对原始的混合信号x'
进行预处理：
         x=x'-E{x'}
这样独立成分也同时变为零均值的量，因为：
         E(s)=A^{-1}E{x}
不过通过这样的中心和预处理，混合矩阵仍可保持不变，因此我们可以放心作为而不用担心
影响对混合矩阵的估计。在完成通过零均值数据对混合矩阵与独立成分的估计工作之后，
被减掉的均值可以通过简单地在独立成分上加A^(-1)E{x'}而恢复。
    
    3.2.6 白化
    给定一些随机变量，通过线性变换将它们转变为相互无关的变量是非常直接的事情。
因此试图采用这样的手段去估计独立变量是一个诱人的想法，这类方法一般称为
(whitenig)或者球面化(sphering)，且经常通过主成分分析(principal component analysis)
来实现。在本节中，我们指出这是不可能的，并讨论了ICA和去相关方法的关系。
我们将会看到，白化事实上可作为ICA中一个有用的预处理技术。
    不相关是独立性的一个弱化的形式。比不相关略强的特性是*白化性*(whitening)。
说一个零均值的随机想了y是白的(或者说是白化的)指它的各分量具有相同的单位方差
且相互不相关，换句话说，y的协方差矩阵(包括相关矩阵)是单位矩阵：
            E{yy^T}=EDE^T
式中，E是E{xx^T}的特征向量的正交矩阵，D是相应的特征向量的对角矩阵，D=diag(d1,...,dn)。
这样，白化过程可以利用下面的白化矩阵来实现：
            V=ED^(-1/2)E^T
式中，矩阵D^(-1/2)只需要通过简单的逐元素开方计算得到D^(-1/2)=diag(d1^(-1/2),...,dn^(-1/2))。
这样得到的白化矩阵记为E{xx^T}^(-1/2)或C^(-1/2)。还有一种可以实现白化的方法是
主成分分析，也可给出相关的白化矩阵。
    白化过程将混合矩阵变换成一个新的矩阵~A，由式(***)和(***)可知：
	      z=VAs=~As
有人也许会希望白化过程能解决ICA问题，因为白化或者不相关与独立性是相关联的，
但事实却并非如此。不相关的条件比独立要弱，且仅由此估计ICA模型是不够的。为了说明
这个问题，考虑对于z的一个正交变换：
       y=Uz
由于U具有正交性，可知：
    E(yy^T)=~AE(ss^T)~A^T=~A~A^T=I
这就意味着我们可以把对混合矩阵的搜索范围限制到正交矩阵的空间中。我们可以无须估计
原始矩阵的A的全部n^2个参数(矩阵项)，只要估计一个正交混合矩阵~A即可。一个正交矩阵
包含了n(n-1)/2个自由度。举例来说二维正交变换仅仅由一个角度参量就能确定。在更高维
的情况下，正交矩阵包含的可变参数个数基本上只有任意矩阵参数个数的一半。
    因此我们可以说白化过程解决了ICA问题的一半。由于白化是一个非常简单的标准过程，
比任何ICA算法都要简单，故通过这样的方式降低问题的复杂程度应该是一个不错的想法。
剩下的一半参数必须通过其它手段去估计，其中一些方法在下面的章节中介绍。
    在本文的很多章节中，我们都假定数据已经被白化处理过，并把这样的数据定义为z。
即使在那些并没有明确白化需求的情况，我们仍然建议进行白化处理，因为该过程减少了
自由参量的个数并明显提高算法的效率，尤其在高维数据的情况下。
   
    3.2.7 时间滤波作为预处理
	因为对于时间序列的信号我们可以对它进行任意的线性滤波，因为它并不改变ICA模型，
当然混合矩阵也不会改变，其中有很多滤波方法。
    1) 低通滤波：是平滑数据的一种方式，经常用来降噪。基本ICA模型噪声的影
响或多或少地被忽略了。基本ICA模型对于没有很多噪声的数据效果较好，因此降噪是有用的，
有时甚至是必须的。
    2) 高通滤波和新息：高通滤波是从数据中去掉缓慢变化的趋势，其在某些情况下，
会增加独立成分的独立性，因此在ICA中可能是有用的。高通滤波一个更原则性的方法是按照
新息(innovation)过程来考虑。新息比原始过程常常更独立，而且可以认为新息通常比原始
过程非高斯性更强。高通滤波一个可能的问题是，可能会增加噪声。
    3) 最优滤波：前面所讲的两种滤波都各自有优缺点。最优滤波应该增加成分的独立性，
同时降低噪声。为达到这一点，在高通和低通滤波之间做某些折中可能会是最好的办法。
这导致带通滤波，它把最高和最低的频率成分滤掉，剩下中间一个合适的频带。至于频带
怎么选择无法给出一个一般的答案，它应该依赖于数据。
    除了简单的低通/高通滤波，还可以有更复杂的方法。例如，可以用(一维)小波变换(***)，
也可以其它时频分解方法。
    
   3.2.8 用PCA进行预处理
   一个对多维数据降维的常用预处理方法是主成分分析(PCA)。基本上，数据被先行投影到
一个子空间：
           ~x=En x
并保留了最多的信息(在最小二乘的意义上)。这样做的好处有使混合矩阵成为方阵以及降噪
和防止过学习等。

   
    3.3 极大非高斯性的ICA估计方法
	现在我们将从中心极限定理出发来直接考察最大非高斯性，首先引入四阶累积量
(或称为峭度)作为非高斯性的一种可操作的度量方式，并利用峭度导出了梯度算法和不动点
算法。然后，为了解决峭度本身带来的一些问题，又引入了信息论中称为负熵(negentropy)
的量作为非高斯性的另一种度量方式，利用该度量也导出了对应的算法。
    
    3.3.1 非高斯就是独立的
    中心极限定理是概率论中的一个经典结论，该定理告诉我们，在一定条件下，独立的
随机变量之和的分布趋向于高斯分布。或者不那么严格地讲，可以认为两个独立随机变量
之和形成的分布比两个原始的随机变量中的任意一个更接近于高斯分布。
    我们现在假定数据向量x是按照ICA数据模型形成其分布：
	     x=As
即该向量是独立成分的混合。为了更清楚地说明问题，我们先在此假定所有的独立成分
具有相同的分布。对独立成分的估计可以通过寻找混合变量的合适的线性组合方式来实现，
因为我们可以对上述混合公式求逆，而表示成一种线性求和的形式：
         s=A^(-1)x
因此为了估计其中一个独立成分，可以考虑对xi进行某种线性组合。用y=b^Tx=xxxxxx
表示该组合，其中b是待确定的向量。注意到同时可以推出y=b^TAs，故y是si的某种线性组合，
而系数则由b^TA给出。将此系数想了记为q，可以得到：
        y=b^Tx=q^Ts=xxxxxx
如果b是A的逆的一行，那么该线性组合b^TA就刚好等于其中一个独立成分，而对应的向量q
只有一个元素为1,其它的元素均为0。
    
   3.3.2 用峭度来度量非高斯性
   1) 峭度及其特性
    为了在ICA中使用非高斯性，我们必须对随机变量的非高斯性定义一个定量化指标。
峭度便是一种经典的非高斯性度量指标。峭度是随机变量的四阶累积量的另一种叫法，
而关于累积量的一般性讨论参见2.7节。这样我们得到了一种估计方法，可以认为该方法
是经典矩方法的一种变形。
    y的峭度kurt(y)可定义为：
	   kurt(y)=E{y^4}-3(E{y^2})^2
注意上式中的所有随机变量都假定为零均值的。对于一般(非零均值)的情况，峭度的定义
则略为复杂些。为了简化问题，我们还可以进一步假设y已经被标准化过，其方差为1：
E{y^2}=1。这样峭度定义公式的右边简化为E{y^4}-3。在说明，峭度实际上就是四阶矩的
一种规范化形式。对于高斯分布的变量y，其四阶矩等于3(E{y^2})^2，因此高斯变量的峭度
为0。对于大部分(但不是所有的)非高斯随机变量，峭度为非零值。
    峭度的值可正可负，具有负峭度的随机变量成为次高斯(subgaussian)的，而那些峭度
为正的随机变量称为超高斯的(supergaussian)的。超高斯随机变量的概率密度函数一般是
“尖顶厚边”的形状，而次高斯分布随机变量的概率密度函数一般是“扁平”形状。
     非高斯性通常可由峭度的绝对值来度量，也可使用峭度的平方。这样的度量对于
高斯变量的取值为零，而对于大部分(而不是全部，但可以认为特例非常少)的非高斯随机
变量的值大于零。
     峭度或其绝对值已在ICA和相关领域广泛地用作非高斯性的度量，这主要是因其无论
从计算上还是理论上都非常简单。从计算的角度，峭度可以简单地使用本数据的四阶矩来
估计(如果样本数据的方差保持不变)。理论分析也因下面的线性特点而变得简单：
       如果x1,x2是两个独立的随机变量，则下面的两式恒成立：
	      xxxxxxxxx
		  xxxxxxxxxxxxxxxx
	   式中，alpah是常数。xxxxxxxxxxxx
   
	2) 采用峭度的梯度算法
        
    在实践中，为了极大化峭度的绝对值，我们可以从某个向量w开始，一句可用的样本值
z(1)，z(2)，...，z(T)，计算出使y=w^Tz的峭度绝对值增大最快的方向，然后将向量w转到
该方向。这种思路可用梯度法及其扩展来实现。
    根据梯度的定义，w^Tz绝对值的梯度可以用下式简单德计算得到：
	  xxxxxxxxxxxxxxx
注意，对于白化过的数据，有E{(w^Tz)^2}=||w||^2。由于我们是在单位球||w||^2=1上进行
优化，梯度法必须进行一定的补充，即在每一步运算后将w投影到单位球上，实际上只要简单
地将w除以其范数即可。
    因此我们得到下面的梯度算法：
	  xxxxxxxxxxxx
	  xxxxxxxxxxxxxxxxx
同时还可以得到该算法的在线(on-line)或自适应版本。这可以通过将算法第二项[xxx]的
数学期望运算忽略而实现，可以得到：
    xxxxxxxxxxxxxx
	xxxxxxxxxxxxxxxx
这样，每个观测z(t)都可以直接在算法中使用。但是必须注意，在计算sign(kurt(w^Tz))项时
桥读定义式中的数学期望算子是不能忽略的。相反，峭度必须通过时间均值来正确地估计。
当然，该时间均值是可以在线估计的。用符号gama表示峭度的估计，我们可以用：
    xxxxxxxxxxxxx
该式给出了一种运行平均(running average)形式的峭度估计。
    实际上，在很多情形下我们是事先知道独立成分的分布类型，清楚它们到底是次高斯
还是超高斯的。这样在算法中就无须再行估计。简单地加入合理的正负符号即可。
     
    3) 峭度的快速不动点算法

	上一小节所述的梯度法与人工神经网络中的学习具有机密的联系，其优势在有输入z(t)
可以直接用于计算，因此在非平稳的环境下能够快速达到自适应。然而，相应的代价则是
算法收敛慢，而且以来于合理的学习速度序列的选择，如果学习速度选择不当，收敛性甚至
会被破坏。因此，我们需要能够使学习速度和可靠性在根本上能够显著提高的新途径，而
不动点迭代算法就是这样一种选择。
    为了得到更为有效的不动点迭代，我们注意到在梯度算法的一个稳定(收敛)点处，梯度
必须指向w的方向，也就是说梯度必须等于一个常数标量与w的乘积。只有在这种情况下，将
梯度与w相加才不改变其方向，且算法在此处收敛(这意味着每次标准化为单位范数后，w除了
符号有可能变化外，其量值不再改变)。[****]令(***)中峭度的梯度与w相等,可以得到:
             xxxxxxxxxxxxx
由此公式直接隐含着一个不动点算法,我们可以首先计算右面的项,并将其赋给w作为新值:
           xxxxxxxxxxx
每次不动点迭代后,w都要除以其番薯以满足相应的约束(||w||=1恒满足，因此在式(***)
可以忽略该项)。最后收敛的向量w以w^Tz的线性组合形式可以给我们其中一个独立成分。
另外，在实际现实中，式(****)中的期望运算只能用相应的估计值来代替。
    注意，不动点迭代的收敛意味着w在一个迭代前后应具有相同的方向，即迭代前w值与
迭代后w值的点积(几乎)等于1。我们并不需要向量最终必须收敛于一个单点，因为w与-w定义
的方向实际是相同的，另外这还因为前面我们已经提到过的事实：由独立成分的定义，我们
无法确定出其正负号。
    事实上，上述的不动点算法虽然简单，但却是非常有效的。它能快速且可靠地收敛。
该不动点算法也称为FastICA(***)。FastICA算法具有一些特点，使其在大部分情况下比基于
梯度的算法具有明显的优势。首先，可以证明(***)该算法的收敛类型是非常快速的立方阶次的。
其次，与基于梯度的算法相比，在算法中没有学习速度或其它可调节的参数，因FastICA更
易用且更可靠。梯度算法似乎只有在必须要对变化环境具有快速自适应的情况下有优势。

    3.3.3 用负熵度量非高斯性

	1) 峭度的缺点
	在前面几节里我们讨论了如何用桥读来度量非高斯性，并得到了一种简单的ICA估计算法。
然而在实际应用中，因为其值只能从测量样本中估计，使得峭度方法也存在一些缺点。主要
问题是，桥读可能对野值(outliers)及其敏感。举例来说，假定在随机变量(当然，这里可能
是指一个具有零均值和单位方差的随机变量)的1000个样本值中，有一个为10,那么峭度最少
为10^4/1000-3=7，也就是说一个单样本值可以使得峭度变得很大。这样我们可以看到，峭度
值可能只取决于分布于边缘的少量观测值，而这些观测可能是错误的或与问题无关的。
换句话说，峭度并不是非高斯性的一个鲁棒度量。
    因此在某些情况下，对于非高斯性的其他度量指标可能比峭度更好。在本节中，我们将
考虑将负熵(negentropy)作为非高斯性的第二个重要的度量，负熵在很多方面都与峭度特性
相反：其鲁棒性好但是计算复杂。
    
    2) 负熵的定义
	负熵的概念来自于微分熵这个信息论参量，这里我们简单地将微分熵成为熵。
熵是信息论的基本概念，随机变量的熵与对该变量的观测能给出的信息量有关。变量越“随机”
(这里的“随机”是指不可预测和无结构)，熵就越大。一个密度为xxx的随机向量，其微分熵
定义为：
        xxxxxxxxxxxxxxxxx
信息论中的一个基本结果指出：在具有相同方差的所有随机变量中，高斯变量具有最大的熵。
这表明熵可以作为非高斯性的一种度量。事实上，上述结论也说明高斯分布是在所有分布中
“最随机”或者说是结构最少的一种分布。对于那些样本明显集中在某些值处的分布，即变量
具有明显聚集性或其pdf非常“尖”的分布，则其熵比较小。
    为了导出合理的非高斯性度量，使其为非负的量值，且对高斯变量其取值为零，
我们可以利用一种成为负熵的量，它实际上是微分熵的一种标准化版本。负熵J定义如下：
       xxxxxxxxxxxxxx
其中xxxx是与y有相同相关(和协方差)矩阵的高斯随机向量。由上面已提到的熵的特性可知，
负熵总是非负的，当且仅当y具有高斯分布时其值为零。负熵还有个有趣的特性：它在可逆
线性变换下是不变的(***)。
    使用负熵(或者等价地说微分熵)作为非高斯性度量的好处是，在于它具有严格的统计
理论背景。事实上，如果我们仅考虑其统计效能，那么负熵在一定程度上可以说是非高斯性
的最优估计。使用负熵的最大问题在于其计算非常困难。如果使用其定义来估计负熵，
则需要首先对随机变量(可能是非参数化的)pdf进行估计。因此，负熵的简化估计是
非常有用的，下面我们要讨论的就是这个问题。负熵的简化估计将可用于到处实现ICA的
一种有效方法。
    对于基于负熵的独立成分分析也有相应的梯度算法和快速不动点算法。

    3.3.4 估计多个独立成分
    本章中直到现在，我们还只估计出了一个独立成分，这就是上述一些算法
有时被称为“一元”(one unit)算法的原因。原理上，我们可以利用不同的初始点
选择，多次运行算法而找到更多的独立成分，但这显然不是估计多个独立成分的
可靠做法。
    要对极大化非高斯性的基本方法进行扩展，以估计更多独立成分，其关键是
利用下面的特性：在白化空间中，不同独立成分的对应向量wi是正交的。概括地
说，各成分的独立性要求它们必须首先是不相关的，在白化空间中因为有
xxxxxxxxxx，故不相关等价于正交。该特性是下面事实的直接推论：白化后的
混合矩阵可以认为是正交的。wi事实上可由混合矩阵之逆中的相应行来定义，
它等于混合矩阵的对应列，这是因为A具有正交性，即A^(-1)=A^T。
    因此，要想估计多个独立分量，我们需要将任意一元算法运行多遍(或者并行
使用多个计算单元，分别对应于w1,...,wn)，而为了避免不同向量收敛到同一个
极值点，必须在每次迭代之后将w....w进行正交化。
    xxxxxxxxxxxxx
	xxxxxxxxxx
	xxxxxxxxxxxxx


	3.4 其它估计方法
	除了上面提到的极大化非高斯性的方法外，还有其它几种非常重要的估计方法，包括
极大似然估计、极大信息原理、极小化互信息、基于张量的估计方法、基于非线性去相关
和非线性PCA的估计方法等。
    1) 极大似然估计：
	极大似然估计是统计估计领域的一种基本方法，请参考(****)，其可以解释为：
采纳那些使得所得观测量具有最大概率的估计参数值。
    2) 信息极大原理
	信息极大原理(infomax principle)是与极大似然原理具有紧密联系的一个
ICA估计原理(****)。对于具有具有非线性输出的神经网络，该原理的出发点是极大化
输出熵或信息流。可以证明信息极大原理等价于极大似然估计。(***)
    3) 极小化互信息的估计方法
	m个随机变量的互信息定义为：
	   I()=xxxxxxxxxxxxxxx
	其中，H(y)为y的微分熵。
	互信息是随机变量之间依赖性的自然度量，总是非负的，当且仅当变量之间统计独立
时为零。互信息考虑了整个变量的依赖结构，而不像主成分分析和相关方法那样仅仅考虑
协方差。在任何情况下，极小化互信息可以解释为独立性最大化的成分。(***)
	同时互信息与非高斯性和似然估计也有非常紧密的关系。(***)
	4) 张量方法
	张量方法不同于前面章节的ICA估计方法。混合数据的四阶累积张量含有数据所包含
的所有四阶信息：它可以用来定义一个张量，来作为协方差矩阵的推广。然后我们可以
对这个矩阵做特征值分解，其特征向量某种程度上直接给出了白化后数据的混合矩阵。
    张量方法可能是最早成功地求解ICA问题的一类方法，但是张量方法最近已经不怎么
常用了，这主要是出于计算的原因，使得整个特征值分解方法受限于低维空间。而且，
这种方法的统计性质不如使用非多项式累积量或似然函数的方法。然而对低维数据来说，
张量方法是不错的选择，而且可归结为FastICA的幂法也可以用于高维空间。
    5) 基于非线性去相关和非线性PCA的ICA估计方法
    这两种方法是独立成分分析早期研究的主要方法，特别是基于非线性去相关的技术，
Jutten，Herault和Ans使用它成功解决了最初的ICA问题。现在提到这些方法主要出于
历史兴趣，因为现在存在更多有效的ICA方法。
    非线性去相关可以看作二姐方法如白化和主成分分析的推广。另一个和PCA有关的ICA
方法是非线性PCA，某些情况下，其可以得到独立成分，其标准也是与其它标准等价的(***)。
 
    3.5 基本ICA方法的比较
	前面介绍了估计ICA模型的集中不同的指标，包括互信息、似然函数、非高斯度量、
累积量和非线性主成分分析(PCA)指标。每个指标都给出了一个目标函数，对它的优化给出了
ICA的估计。它们都是紧密联系的，实际上，几乎所有这些估计原理都可以认为是相同的一般
指标的不同表现。
    但是这些不同的方法也有一些差异：
	1) 某些原理(特别是最大非高斯性)能够估计单个独立成分，而其它方法需要同时估计
	   所有的成分。
	2) 目标函数使用基于(假设的)独立成分的概率函数的非多项式函数，然而其它的使用和
	   累积量有关的多项式函数。这导致了目标函数中出现了不同的非二次函数。
	3) 在许多估计原理中，对独立成分的估计约束为不相关的。这稍微减少了做估计的空间。
	   考虑一下，如互信息，没有道理恰好分解给出不相关成分时，互信息是最小的。因此，
	   不相关约束略微降低了估计方法理论上的性能。在实际中，这是可以忽略的。
	4) 实际中，一个重要的差异是，在最大似然估计中，独立成分的密度经常是使用先验知识
	   事先固定的。这是可能的，因为对独立成分的概率密度的了解不需要很精确：实际上，
	   知道它们是次高斯的还是超高斯的就足够了。然而如果对于独立成分的先验支持是
	   不正确的，最大似然估计会得到完全错误的结果(***)。所以使用最大似然估计时
	   需要特别小心。作为对比，如果使用负熵的近似，这个问题通常不会发生，因为我们
	   使用的对负熵的近似并不基于对密度的合理近似。因此这些近似不会有太多的问题。
    
	对真实数据，真正的独立成分是未知的，而且标准ICA模型的假设可能不成立，或只是
近似成立。因此只可能在使用的应用中比较ICA算法之间的性能。
    xxxx通过实验给出了下面的一般性结论(****)
	1) ICA是鲁棒的方法。即使统计独立的假设不是严格满足的，算法会收敛到一组明确的成分，
	   或一个维数远小于问题的维数的子空间(卫星数据)。这个好的特征促使ICA成为一个数据
	   分析的一般工具。
    2) FastICA算法和使用自适应非线性函数的自然梯度最大似然算法(ExtBS)对真实数据通常
	   得出相似的结果。这并不使人感到惊奇，因为这些算法之间存在紧密的理论联系。另一对
	   表现相似的算法是EASI算法和使用递归最小二乘的非线性PCA算法(NPCA-NLS)。
	3) 在困难的真实问题中，使用集中不同的ICA算法是有用的，因为它们可能会从数据中得到
       不同的独立成分。
	
	3.6 算法的选择
	出于实用注意的考虑，各位研究者总结了一些可以注意的要点。
	
	3.6.1 应该估计多少个成分
	xxxxxxxxxx给出了一些建议：
	    在实际中，经常出现的另一个问题是决定估计的独立成分的个数。如果只是估计和数据
		维数一样多的成分，就不存在这个问题了，但这并不总是个好主意。
		首先，因为使用PCA降维是经常的事情，必须选择需要保留的主成分的数目。这是一个
		      经典的问题，通常是选择足够解释数据，如保留90%方差的最小数目的主成分。
			  这个维数经常由试探得到，没有理论的指导。
	    其次，由于计算的原因，我们可能只能估计相比数据维数的一小部分独立成分(PCA预
			  处理之后)。这种情况是当前数据的维数很高而我们又不想通过PCA把维数降得
		      很低，因为PCA总有可能丢失独立成分的风险。使用FastICA和其它允许估计一小
			  部分独立成分的算法，我们能够通过ICA来降维。实际上这有点像投影寻踪。
			  这里，更难说应该估计多少成分，可能只好用试探的办法。
		其它确定独立成分个数的指标如信息论，贝叶斯等在(***)中有更多讨论。
	    
     3.6.2 算法选择
	 ICA的多数估计原理和目标函数至少在理论上是等价的。从实用的角度主要根据下面几点来选择：
	   * 要在同时估计所有的独立成分还是只估计一部分(可能是一个一个的)之间做出选择。
	     相应地要在对称和分级去相关之间作出选择。在多数情况下，推荐对称去相关。串行方法
		 主要是在只想估计很少数量的独立成分时或其它特殊情况下有用。串行正交化的主要缺点，
		 是地一个成分的估计误差会累积，并增加下一个成分的误差。
	   * 需要选择算法中使用的非线性函数。好像在多数应用中都更偏向于使用鲁棒的，非多项式
	     的非线性函数。最简单的就是使用双曲正切函数作为非线性函数g。这对FastICA来说已经足够了。
	   * 最后，要在在线和批处理算法中选择。多数情况下，在估计前，整个数据集都是有用的，
	     这在不同的上下文中称为批处理，分块或离线估计。这种情况下可以使用FastICA，这也是
		 我们推荐的算法。在离线的情况下，推荐的是那些由随机梯度方法得到的算法。应该注意
		 在某些情况下，FastICA算法可能不会收敛得很好，因为牛顿类型的算法有时会有震荡行为。
		 这个问题可以通过使用梯度方法或者二者的结合来减轻(****)。
